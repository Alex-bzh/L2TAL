{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e935d968-62d2-4deb-ace6-6c03fb3321af",
   "metadata": {},
   "source": [
    "# Étiquetage morphosyntaxique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a2fb56-3dbe-48ea-a918-f2ca5ed4a2f1",
   "metadata": {},
   "source": [
    "## Définition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb2987a-028a-474a-831e-ec5d33da5667",
   "metadata": {},
   "source": [
    "Opération par laquelle un programme associe automatiquement à un mot des étiquettes grammaticales, comme :\n",
    "- la partie du discours (*N*)\n",
    "- le genre (*NF*)\n",
    "- le nombre (*NFP*)\n",
    "- …\n",
    "\n",
    "Elle intervient après celle de segmentation en mots et se positionne comme pré-requis pour l’analyse syntaxique de surface.\n",
    "\n",
    "Le résultat est un couple (mot, étiquette) :\n",
    "- *Le petit chat boit du lait.*\n",
    "- *Le/DET petit/ADJ chat/N boit/V du/DET lait/N ./PONCT*\n",
    "\n",
    "Certaines étiquettes étant en concurrence pour un mot, leur attribution est rarement aisée. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf468a81-d420-430d-8ec1-56fd2bdbf3d2",
   "metadata": {},
   "source": [
    "### Exemple d’énoncé ambigu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa19244-3aed-4f12-a720-4983dbf67991",
   "metadata": {},
   "source": [
    "Considérons la phrase ci-dessous :\n",
    "\n",
    "> La petite brise la glace.\n",
    "\n",
    "Elle peut s’interpréter de deux façons : soit on imagine une petite fille en colère brisant un miroir, soit on se place dans un contexte hivernal où le vent fait trembler notre héroïne :\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph \"Les déboires d’une aventurière\"\n",
    "        direction TB\n",
    "        A2(\"La\"):::formal-->At2[\"DET\"]:::academic\n",
    "        B2(\"petite\"):::formal-->Bt2[\"ADJ\"]:::academic\n",
    "        C2(\"brise\"):::formal-->Ct2[\"N\"]:::academic\n",
    "        D2(\"la\"):::formal-->Dt2[\"PRO\"]:::academic\n",
    "        E2(\"glace\"):::formal-->Et2[\"V\"]:::academic\n",
    "        F2(\".\"):::formal-->Ft2[\"PONCT\"]:::academic\n",
    "    end\n",
    "    subgraph \"La colère d’une petite fille\"\n",
    "        direction TB\n",
    "        A(\"La\"):::formal-->At1[\"DET\"]:::academic\n",
    "        B(\"petite\"):::formal-->Bt1[\"N\"]:::academic\n",
    "        C(\"brise\"):::formal-->Ct1[\"V\"]:::academic\n",
    "        D(\"la\"):::formal-->Dt1[\"DET\"]:::academic\n",
    "        E(\"glace\"):::formal-->Et1[\"N\"]:::academic\n",
    "        F(\".\"):::formal-->Ft1[\"PONCT\"]:::academic\n",
    "    end\n",
    "    classDef formal fill:#fff,stroke:#D68738,color:#D68738\n",
    "    classDef academic fill:#032B4F,stroke:#032B4F,color:#fff\n",
    "```\n",
    "\n",
    "Si cet énoncé est caractéristique des difficultés qu’un étiqueteur peut rencontrer, la majorité des mots et des phrases du français est ambiguë."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cca41d-a704-4aba-a221-01284b46ca3c",
   "metadata": {},
   "source": [
    "## Étiquetage et décision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec2dce5-ee3e-459c-ade0-a565032aacf6",
   "metadata": {},
   "source": [
    "Pour un étiqueteur, l’analyse de la phrase revient à prendre une décision entre deux voies possibles : l’une ouverte par l’interprétation de *petite* comme un nom et l’autre comme un adjectif.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A(\"La\n",
    "    DET\"):::academic-->B(\"petite\n",
    "    N\"):::formal\n",
    "    B-->C(\"brise\n",
    "    V\"):::formal\n",
    "    C-->D(\"la\n",
    "    DET\"):::formal\n",
    "    D-->E(\"glace\n",
    "    N\"):::formal\n",
    "    E-->F(\".\n",
    "    PONCT\"):::academic\n",
    "    A-->G(\"petite\n",
    "    ADJ\"):::academic\n",
    "    G-->H(\"brise\n",
    "    N\"):::academic\n",
    "    H-->I(\"la\n",
    "    PRO\"):::academic\n",
    "    I-->J(\"glace\n",
    "    V\"):::academic\n",
    "    J-->F\n",
    "    classDef formal fill:#fff,stroke:#D68738,color:#D68738\n",
    "    classDef academic fill:#032B4F,stroke:#032B4F,color:#fff\n",
    "```\n",
    "\n",
    "Ces deux voies ne sont pas équiprobables et l’interprétation du mot *petite* dépendra fortement du corpus d’apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e0c68-63bb-4ec5-bbe3-3a211843486f",
   "metadata": {},
   "source": [
    "### Analyse d’un unigramme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a081efc1-77eb-4945-9915-351cea42d92d",
   "metadata": {},
   "source": [
    "Pour un mot *w* donné, on peut déterminer son étiquette *t* grâce à sa fréquence d’apparition dans le corpus :\n",
    "\n",
    "$$\n",
    "P(t|w) = \\frac{F(w,t)}{F(w)}\n",
    "$$\n",
    "\n",
    "Si le mot *petite* apparaît 38 fois dont 12 en tant que nom, la probabilité de le voir avec l’étiquette *ADJ* est de :\n",
    "\n",
    "$$\n",
    "P(ADJ|petite) = \\frac{26}{38} = 0,6842\n",
    "$$\n",
    "\n",
    "À noter que la probabilité de *t* sachant *w* peut s’écrire comme un rapport entre la probabilité du couple mot/étiquette et la probabilité du mot :\n",
    "\n",
    "$$\n",
    "P(t|w) = \\frac{P(t,w)}{P(w)}\n",
    "$$\n",
    "\n",
    "Dans notre exemple fictif, imaginons que nous avons entraîné notre étiqueteur sur un corpus de 1000 mots. La formule donnerait :\n",
    "\n",
    "$$\n",
    "P(ADJ|petite) = \\frac{\\frac{26}{1000}}{\\frac{38}{1000}} = 0,6842\n",
    "$$\n",
    "\n",
    "De manière réciproque, il serait possible de déduire le mot sachant l’étiquette. Toutefois, la précision dans ce sens sera bien moindre : quand on hésite seulement entre quelques étiquettes pour un mot, la quantité de mots possibles pour une étiquette est bien plus importante.\n",
    "\n",
    "Ce modèle est jugé simpliste : non seulement il choisit parmi plusieurs possibilités la plus probable ($P(ADJ|petite) > P(N|petite)$) mais en plus il ne tient pas compte de l’antériorité. Au mot suivant, *brise*, il pourrait choisir d’attribuer l’étiquette *V*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceed65d-a12d-475f-8dc5-af54ed74e2c5",
   "metadata": {},
   "source": [
    "### Étiquetage *n*-gramme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d4f720-91b0-456c-9a53-38f8ef59cff1",
   "metadata": {},
   "source": [
    "#### Définition de la tâche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63647c20-72da-4181-b309-229a38719ab6",
   "metadata": {},
   "source": [
    "L’idée sous-jacente des étiqueteurs *n*-gramme est que l’étiquette d’un mot dépend des *n-1* mots qui le précèdent. Dans la pratique, comme les ressources nécessaires pour le calcul des probabilités croissent avec *n* et que la fiabilité des résultats diminue, notamment en raison d’un phénomène de dilution, on se contente d’une faible valeur de *n*.\n",
    "\n",
    "Trois hypothèses sont retenues :\n",
    "\n",
    "- l’approche bayésienne  \n",
    "$P(t|w) = \\frac{P(w|t) \\cdot P(t)}{P(w)}$\n",
    "- l’horizon limité de Markov  \n",
    "$P(t_n|t_{1,n-1}) = P(t_n|t_{n-1})$\n",
    "- la probablité d’apparition d’un mot pour une étiquette ne dépend que de son étiquette  \n",
    "$P(w_i|t_{1,n}) = P(w_i|t_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34bea63-fc28-44b7-8b7f-aa87cd72bd5c",
   "metadata": {},
   "source": [
    "#### Paramètres de résolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6b0a9-a398-4d86-b33e-d4c5d02e4e28",
   "metadata": {},
   "source": [
    "Soient deux ensembles *W* et *T* pour les mots et les étiquettes. À une séquence de *n* mots est associée une séquence de *n* étiquettes de telle manière que :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    w_{1,n} &= w_1 w_2 \\ldots w_n\\\\\n",
    "    t_{1,n} &= t_1 t_2 \\ldots t_n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "L’objectif, sachant $w_{1,n}$, est de déterminer $t_{1,n}$ en maximisant $P(t_{1,n}|w_{1,n})$, c’est-à-dire, après avoir calculé toutes les probabilités possibles, de ne conserver que la plus forte :\n",
    "\n",
    "$$\n",
    "t_{1,n} = max_{t_{1,n}} P(t_{1,n}|w_{1,n})\n",
    "$$\n",
    "\n",
    "Ce qui, d’après le théorème de Bayes, s’exprime en renversant la condition :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    P(t_{1,n}|w_{1,n}) &= \\frac{P(w_{1,n}|t_{1,n}) \\cdot P(t_{1,n})}{P(w_{1,n})}\\\\\n",
    "    t_{1,n} &= \\text{max} P(w_{1,n}|t_{1,n}) \\cdot P(t_{1,n})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Il reste deux probabilités à calculer pour lesquelles on applique les hypothèses retenues plus haut :\n",
    "\n",
    "- $P(w_{1,n}|t_{1,n})$ : hypothèse selon laquelle un mot ne dépend que de son étiquette\n",
    "- $P(t_{1,n})$ : hypothèse selon laquelle une étiquette dépend de ses *n* étiquettes précédentes\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    P(w_{1,n}|t_{1,n}) &= P(w_1|t_1) \\cdot P(w_2|t_2) \\ldots P(w_n|t_n)\\\\\n",
    "    P(t_{1,n}) &= P(t_1) \\cdot P(t_2|t_1) \\cdot P(t_3|t_2) \\ldots P(t_n|t_{n−1})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e00cef-1bc6-44c0-ae3e-ccbdcbcdd2ff",
   "metadata": {},
   "source": [
    "### Modèle de Markov caché d’ordre *N*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbdadd-8c4e-4012-8de6-a013ebdbd005",
   "metadata": {},
   "source": [
    "Le modèle décrit plus haut est appelé modèle de Markov caché d’ordre 1 (*Hidden Markov Model*, ou HMM) dans la mesure où la probabilité d’apparition d’une étiquette ne dépend que de la précédente. Pour un ordre plus élevé, par exemple pour un HMM d’ordre 2, nous aurons :\n",
    "\n",
    "$$\n",
    "P(t_{1,n}) = P(t_1) \\cdot P(t_2|t_1) \\cdot P(t_3|t_1 t_2) \\ldots P(t_n|t_{n−2}t_{n−1})\n",
    "$$\n",
    "\n",
    "En appliquant un HMM, la tâche pour un étiquetage *n*-gramme se formulerait ainsi :\n",
    "\n",
    "$$\n",
    "t_{1,n} = \\text{max} P(w_1|t_1) \\cdot \\prod_{i=1,n} P(t_i|t_{i-1}) \\cdot P(w_i|t_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d26945f-1345-4e72-afb7-f3c9ea466d46",
   "metadata": {},
   "source": [
    "### Estimation des probabilités avec un corpus d’apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb229d-fa96-45aa-8f06-4cb60bf7224f",
   "metadata": {},
   "source": [
    "La première étape consiste à calculer les probabilités d’émission pour chaque mot sachant son étiquette :\n",
    "\n",
    "$$\n",
    "P(w_i|t_i) = \\frac{F(w_i, t_i)}{F(t_i)}\n",
    "$$\n",
    "\n",
    "Ensuite, on passe aux probabilités de transition :\n",
    "\n",
    "$$\n",
    "P(t_i|t_{i-1}) = \\frac{F(t_{i-1} t_i)}{F(t_{i-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88779a0b-fbbb-48fa-9b7c-c278dba181ea",
   "metadata": {},
   "source": [
    "## Étiquetage manuel avec un HMM d’ordre 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef064ec5-1aea-4bfa-a95c-1601995f6ce0",
   "metadata": {},
   "source": [
    "Soit la phrase à étiqueter :\n",
    "\n",
    "> Lemmy joue de la basse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1ed3c-6009-4431-8412-5fbc35cdcc03",
   "metadata": {},
   "source": [
    "### Apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d1670-799b-4bf7-add5-5a517c9bee4d",
   "metadata": {},
   "source": [
    "Un corpus d’apprentissage fictif nous permet de dresser le tableau des probabilités d’apparition suivant :\n",
    "\n",
    "|$w, t$|ADJ|ART|NC|NP|PRE|PRO|V|\n",
    "|:-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|**Lemmy**|0|0|0|0.0001|0|0|0|\n",
    "|**joue**|0|0|0.06|0|0|0|0.02|\n",
    "|**de**|0|0.102|0|0|0.013|0|0|\n",
    "|**la**|0|0.12|0.0002|0|0|0.14|0|\n",
    "|**basse**|0.0015|0|0.0002|0|0|0|0|\n",
    "\n",
    "Ainsi que, pour les bigrammes potentiels :\n",
    "\n",
    "|$t|t_{-1}$|ADJ|ART|NC|NP|PRE|PRO|V|\n",
    "|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|**ADJ**|0|0.0001|0.34|0|0|0.001|0|\n",
    "|**ART**|0|0.32|0.09|0|0.63|0|0.51|\n",
    "|**NC**|0|0.18|0.02|0.0001|0.001|0.03|0|\n",
    "|**NP**|0|0|0|0|0|0|0|\n",
    "|**PRE**|0|0|0.23|0|0|0|0.0001|\n",
    "|**PRO**|0|0.001|0|0|0.22|0|0|\n",
    "|**V**|0|0|0|0.48|0|0|0|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7220ecad-6e1f-4846-85e6-294eb3ab34a6",
   "metadata": {},
   "source": [
    "### Arbre des possibilités"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd83cc4-9aa2-4620-870e-379cdaaeb7a1",
   "metadata": {},
   "source": [
    "Les calculs combinatoires étant trop nombreux, nous implémentons un arbre de décision pour les besoins de l’exercice.\n",
    "\n",
    "Ci-dessous l’arbre des étiquetages possibles avec la séquence privilégiée (*Lemmy/NP joue/V de/ART la/ART basse/NC*) :\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "\n",
    "    A(\"NP\"):::academic-->B1(\"NC\"):::formal\n",
    "    A-->B2(\"V\"):::academic\n",
    "\n",
    "    B1-->C1(\"ART\"):::formal\n",
    "    B1-->C2(\"PRE\"):::formal\n",
    "\n",
    "    B2-->C3(\"ART\"):::academic\n",
    "    B2-->C4(\"PRE\"):::formal\n",
    "\n",
    "    C1-->D1(\"ART\"):::formal\n",
    "    C1-->D2(\"NC\"):::formal\n",
    "    C1-->D3(\"PRO\"):::formal\n",
    "    \n",
    "    C2-->D4(\"ART\"):::formal\n",
    "    C2-->D5(\"NC\"):::formal\n",
    "    C2-->D6(\"PRO\"):::formal\n",
    "    \n",
    "    C3-->D7(\"ART\"):::academic\n",
    "    C3-->D8(\"NC\"):::formal\n",
    "    C3-->D9(\"PRO\"):::formal\n",
    "    \n",
    "    C4-->D10(\"ART\"):::formal\n",
    "    C4-->D11(\"NC\"):::formal\n",
    "    C4-->D12(\"PRO\"):::formal\n",
    "\n",
    "    D1-->E1(\"ADJ\"):::formal\n",
    "    D1-->E2(\"NC\"):::formal\n",
    "\n",
    "    D2-->E3(\"ADJ\"):::formal\n",
    "    D2-->E4(\"NC\"):::formal\n",
    "\n",
    "    D3-->E5(\"ADJ\"):::formal\n",
    "    D3-->E6(\"NC\"):::formal\n",
    "\n",
    "    D4-->E7(\"ADJ\"):::formal\n",
    "    D4-->E8(\"NC\"):::formal\n",
    "\n",
    "    D5-->E9(\"ADJ\"):::formal\n",
    "    D5-->E10(\"NC\"):::formal\n",
    "\n",
    "    D6-->E11(\"ADJ\"):::formal\n",
    "    D6-->E12(\"NC\"):::formal\n",
    "\n",
    "    D7-->E13(\"ADJ\"):::formal\n",
    "    D7-->E14(\"NC\"):::academic\n",
    "\n",
    "    D8-->E15(\"ADJ\"):::formal\n",
    "    D8-->E16(\"NC\"):::formal\n",
    "\n",
    "    D9-->E17(\"ADJ\"):::formal\n",
    "    D9-->E18(\"NC\"):::formal\n",
    "\n",
    "    D10-->E19(\"ADJ\"):::formal\n",
    "    D10-->E20(\"NC\"):::formal\n",
    "\n",
    "    D11-->E21(\"ADJ\"):::formal\n",
    "    D11-->E22(\"NC\"):::formal\n",
    "\n",
    "    D12-->E23(\"ADJ\"):::formal\n",
    "    D12-->E24(\"NC\"):::formal\n",
    "\n",
    "    classDef formal fill:#fff,stroke:#D68738,color:#D68738\n",
    "    classDef academic fill:#032B4F,stroke:#032B4F,color:#fff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bddb74-0faa-4476-b08a-ea08b331ab9d",
   "metadata": {},
   "source": [
    "### Calcul des probabilités"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121b515-a5ed-44ff-89ba-bd3abcdede2e",
   "metadata": {},
   "source": [
    "Sachant que Lemmy ne peut être étiqueté qu’avec *NP*, la question revient à connaître la probabilité de la séquence *joue de la basse*. Nous identifions les bigrammes suivants : *Lemmy joue*, *joue de*, *de la* et *la basse*. Pour chacun, il convient de calculer toutes les possibilités d’étiquetage et conserver la valeur maximale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95090517-28b2-42ae-854d-480647b2f2d1",
   "metadata": {},
   "source": [
    "#### Bigramme *Lemmy joue*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc9c15-7d5a-4086-889b-a0c3dbfcbee7",
   "metadata": {},
   "source": [
    "1. Séquence *NP, NC* :\n",
    "   $$\n",
    "   \\begin{align}\n",
    "   &= P(Lemmy|NP) \\times P(joue|NC) \\times P(NC|NP)\\\\\n",
    "   &= 0.0001 \\times 0.06 \\times 0.0001\\\\\n",
    "   &= 6 \\times 10^{-10}\n",
    "   \\end{align}\n",
    "   $$\n",
    "2. Séquence *NP, V* :\n",
    "   $$\n",
    "   \\begin{align}\n",
    "   &= P(Lemmy|NP) \\times P(joue|V) \\times P(V|NP)\\\\\n",
    "   &= 0.0001 \\times 0.02 \\times 0.48\\\\\n",
    "   &= 9.6 \\times 10^{-7}\n",
    "   \\end{align}\n",
    "   $$\n",
    "\n",
    "À ce stade, l’étiquetage privilégié est :\n",
    "\n",
    "> Lemmy/NP joue/V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7a193-e425-4e70-b609-a793530353fd",
   "metadata": {},
   "source": [
    "#### Bigramme *joue de*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd69f62d-4f1d-40cc-b675-c8e7d7a86cae",
   "metadata": {},
   "source": [
    "1. Séquence *V, ART* :\n",
    "   $$\n",
    "   \\begin{align}\n",
    "   &= P(joue|V) \\times P(de|ART) \\times P(V|NP) \\times P(ART|V)\\\\\n",
    "   &= 0.02 \\times 0.112 \\times 0.48 \\times 0.51\\\\\n",
    "   &= 5.48352 \\times 10^{-4}\n",
    "   \\end{align}\n",
    "   $$\n",
    "2. Séquence *V, PRE* :\n",
    "   $$\n",
    "   \\begin{align}\n",
    "   &= P(joue|V) \\times P(de|PRE) \\times P(V|NP) \\times P(PRE|V)\\\\\n",
    "   &= 0.02 \\times 0.013 \\times 0.48 \\times 0.0001\\\\\n",
    "   &= 1.248 \\times 10^{-8}\n",
    "   \\end{align}\n",
    "   $$\n",
    "\n",
    "À ce stade, l’étiquetage privilégié est :\n",
    "\n",
    "> Lemmy/NP joue/V de/ART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c612b-8568-44f3-9135-2e48aba802cc",
   "metadata": {},
   "source": [
    "#### Bigramme *de la*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbce9d2-c33e-4624-987d-1686ed521bfd",
   "metadata": {},
   "source": [
    "1. Séquence *ART, ART* :\n",
    "   $$\n",
    "   \\begin{align}\n",
    "   &= P(de|ART) \\times P(la|ART) \\times P(ART|V) \\times P(ART|ART)\\\\\n",
    "   &= 0.102 \\times 0.12 \\times 0.51 \\times 0.32\\\\\n",
    "   &= 1.997 \\times 10^{-3}\n",
    "   \\end{align}\n",
    "   $$\n",
    "2. Séquence *ART, NC* :\n",
    "   $$\n",
    "   \\begin{align}\n",
    "   &= P(de|ART) \\times P(la|NC) \\times P(ART|V) \\times P(NC|ART)\\\\\n",
    "   &= 0.102 \\times 0.0002 \\times 0.32 \\times 0.18\\\\\n",
    "   &= 1.175 \\times 10^{-6}\n",
    "   \\end{align}\n",
    "   $$\n",
    "3. Séquence *ART, PRO* :\n",
    "   $$\n",
    "   \\begin{align}\n",
    "   &= P(de|ART) \\times P(la|PRO) \\times P(ART|V) \\times P(PRO|ART)\\\\\n",
    "   &= 0.102 \\times 0.14 \\times 0.51 \\times 0.001\\\\\n",
    "   &= 7.2828 \\times 10^{-6}\n",
    "   \\end{align}\n",
    "   $$\n",
    "\n",
    "À ce stade, l’étiquetage privilégié est :\n",
    "\n",
    "> Lemmy/NP joue/V de/ART la/ART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1b81a-0cdb-4454-9381-051876a4e92f",
   "metadata": {},
   "source": [
    "#### Bigramme *la basse*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c098c1-90f4-4996-b99b-46834ed04207",
   "metadata": {},
   "source": [
    "1. Séquence *ART, ADJ* :\n",
    "   $$\n",
    "   \\begin{align}\n",
    "   &= P(la|ART) \\times P(basse|ADJ) \\times P(ART|ART) \\times P(ADJ|ART)\\\\\n",
    "   &= 0.12 \\times 0.0015 \\times 0.32 \\times 0.0001\\\\\n",
    "   &= 5.76 \\times 10^{-9}\n",
    "   \\end{align}\n",
    "   $$\n",
    "2. Séquence *ART, NC* :\n",
    "   $$\n",
    "   \\begin{align}\n",
    "   &= P(la|ART) \\times P(basse|NC) \\times P(ART|ART) \\times P(NC|ART)\\\\\n",
    "   &= 0.12 \\times 0.0002 \\times 0.32 \\times 0.18\\\\\n",
    "   &= 1.3824 \\times 10^{-5}\n",
    "   \\end{align}\n",
    "   $$\n",
    "\n",
    "À présent, nous pouvons conclure que l’étiquetage le plus probable est :\n",
    "\n",
    "> Lemmy/NP joue/V de/ART la/ART basse/NC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
