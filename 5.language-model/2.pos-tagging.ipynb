{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e935d968-62d2-4deb-ace6-6c03fb3321af",
   "metadata": {},
   "source": [
    "# Étiquetage morphosyntaxique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a2fb56-3dbe-48ea-a918-f2ca5ed4a2f1",
   "metadata": {},
   "source": [
    "## Définition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb2987a-028a-474a-831e-ec5d33da5667",
   "metadata": {},
   "source": [
    "Opération par laquelle un programme associe automatiquement à un mot des étiquettes grammaticales, comme :\n",
    "- la partie du discours (*N*)\n",
    "- le genre (*NF*)\n",
    "- le nombre (*NFP*)\n",
    "- …\n",
    "\n",
    "Elle intervient après celle de segmentation en mots et se positionne comme pré-requis pour l’analyse syntaxique de surface.\n",
    "\n",
    "Le résultat est un couple (mot, étiquette) :\n",
    "- *Le petit chat boit du lait.*\n",
    "- *Le/DET petit/ADJ chat/N boit/V du/DET lait/N ./PONCT*\n",
    "\n",
    "Certaines étiquettes étant en concurrence pour un mot, leur attribution est rarement aisée. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf468a81-d420-430d-8ec1-56fd2bdbf3d2",
   "metadata": {},
   "source": [
    "### Exemple d’énoncé ambigu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa19244-3aed-4f12-a720-4983dbf67991",
   "metadata": {},
   "source": [
    "Considérons la phrase ci-dessous :\n",
    "\n",
    "> La petite brise la glace.\n",
    "\n",
    "Elle peut s’interpréter de deux façons : soit on imagine une petite fille en colère brisant un miroir, soit on se place dans un contexte hivernal où le vent fait trembler l’héroïne :\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph \"Les déboires d’une aventurière\"\n",
    "        direction TB\n",
    "        A2(\"La\"):::formal-->At2[\"DET\"]:::academic\n",
    "        B2(\"petite\"):::formal-->Bt2[\"ADJ\"]:::academic\n",
    "        C2(\"brise\"):::formal-->Ct2[\"N\"]:::academic\n",
    "        D2(\"la\"):::formal-->Dt2[\"PRO\"]:::academic\n",
    "        E2(\"glace\"):::formal-->Et2[\"V\"]:::academic\n",
    "        F2(\".\"):::formal-->Ft2[\"PONCT\"]:::academic\n",
    "    end\n",
    "    subgraph \"La colère d’une petite fille\"\n",
    "        direction TB\n",
    "        A(\"La\"):::formal-->At1[\"DET\"]:::academic\n",
    "        B(\"petite\"):::formal-->Bt1[\"N\"]:::academic\n",
    "        C(\"brise\"):::formal-->Ct1[\"V\"]:::academic\n",
    "        D(\"la\"):::formal-->Dt1[\"DET\"]:::academic\n",
    "        E(\"glace\"):::formal-->Et1[\"N\"]:::academic\n",
    "        F(\".\"):::formal-->Ft1[\"PONCT\"]:::academic\n",
    "    end\n",
    "    classDef formal fill:#fff,stroke:#D68738,color:#D68738\n",
    "    classDef academic fill:#032B4F,stroke:#032B4F,color:#fff\n",
    "```\n",
    "\n",
    "Si cet énoncé est caractéristique des difficultés qu’un étiqueteur peut rencontrer, la majorité des mots et des phrases du français est ambiguë."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cca41d-a704-4aba-a221-01284b46ca3c",
   "metadata": {},
   "source": [
    "## Étiquetage et décision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec2dce5-ee3e-459c-ade0-a565032aacf6",
   "metadata": {},
   "source": [
    "Pour un étiqueteur, l’analyse de la phrase revient à prendre une décision entre deux voies possibles : l’une ouverte par l’interprétation de *petite* comme un nom et l’autre comme un adjectif.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A(\"La\n",
    "    DET\"):::academic-->B(\"petite\n",
    "    N\"):::formal\n",
    "    B-->C(\"brise\n",
    "    V\"):::formal\n",
    "    C-->D(\"la\n",
    "    DET\"):::formal\n",
    "    D-->E(\"glace\n",
    "    N\"):::formal\n",
    "    E-->F(\".\n",
    "    PONCT\"):::academic\n",
    "    A-->G(\"petite\n",
    "    ADJ\"):::academic\n",
    "    G-->H(\"brise\n",
    "    N\"):::academic\n",
    "    H-->I(\"la\n",
    "    PRO\"):::academic\n",
    "    I-->J(\"glace\n",
    "    V\"):::academic\n",
    "    J-->F\n",
    "    classDef formal fill:#fff,stroke:#D68738,color:#D68738\n",
    "    classDef academic fill:#032B4F,stroke:#032B4F,color:#fff\n",
    "```\n",
    "\n",
    "Ces deux voies ne sont pas équiprobables et l’interprétation du mot *petite* dépendra fortement du corpus d’apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e0c68-63bb-4ec5-bbe3-3a211843486f",
   "metadata": {},
   "source": [
    "### Analyse d’un unigramme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a081efc1-77eb-4945-9915-351cea42d92d",
   "metadata": {},
   "source": [
    "Pour un mot *w* donné, on peut déterminer son étiquette *t* grâce à sa fréquence d’apparition dans le corpus :\n",
    "\n",
    "$$\n",
    "P(t|w) = \\frac{F(w,t)}{F(w)}\n",
    "$$\n",
    "\n",
    "Si le mot *petite* apparaît 38 fois dont 12 en tant que nom, la probabilité de le voir avec l’étiquette *ADJ* est de :\n",
    "\n",
    "$$\n",
    "P(ADJ|petite) = \\frac{26}{38} = 0,6842\n",
    "$$\n",
    "\n",
    "À noter que la probabilité de *t* sachant *w* peut s’écrire comme un rapport entre la probabilité du couple mot/étiquette et la probabilité du mot :\n",
    "\n",
    "$$\n",
    "P(t|w) = \\frac{P(t,w)}{P(w)}\n",
    "$$\n",
    "\n",
    "Dans notre exemple fictif, imaginons que nous avons entraîné notre étiqueteur sur un corpus de 1000 mots. La formule donnerait :\n",
    "\n",
    "$$\n",
    "P(ADJ|petite) = \\frac{\\frac{26}{1000}}{\\frac{38}{1000}} = 0,6842\n",
    "$$\n",
    "\n",
    "De manière réciproque, il serait possible de déduire le mot sachant l’étiquette. Toutefois, la précision dans ce sens sera bien moindre : quand on hésite seulement entre quelques étiquettes pour un mot, la quantité de mots possibles pour une étiquette est bien plus importante.\n",
    "\n",
    "Ce modèle est jugé simpliste : non seulement il choisit parmi plusieurs possibilités la plus probable ($P(ADJ|petite) > P(N|petite)$) mais en plus il ne tient pas compte de l’antériorité. Au mot suivant, *brise*, il pourrait choisir d’attribuer l’étiquette *V*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceed65d-a12d-475f-8dc5-af54ed74e2c5",
   "metadata": {},
   "source": [
    "### Étiquetage *n*-gramme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d4f720-91b0-456c-9a53-38f8ef59cff1",
   "metadata": {},
   "source": [
    "#### Définition de la tâche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63647c20-72da-4181-b309-229a38719ab6",
   "metadata": {},
   "source": [
    "L’idée sous-jacente des étiqueteurs *n*-gramme est que l’étiquette d’un mot dépend des *n-1* mots qui le précèdent. Dans la pratique, comme les ressources nécessaires pour le calcul des probabilités croissent avec *n* et que la fiabilité des résultats diminue, notamment en raison d’un phénomène de dilution, on se contente d’une faible valeur de *n*.\n",
    "\n",
    "Trois hypothèses sont retenues :\n",
    "\n",
    "- l’approche bayésienne  \n",
    "$P(t|w) = \\frac{P(w|t) \\cdot P(t)}{P(w)}$\n",
    "- l’horizon limité de Markov  \n",
    "$P(t_n|t_{1,n-1}) = P(t_n|t_{n-1})$\n",
    "- la probablité d’apparition d’un mot pour une étiquette ne dépend que de son étiquette  \n",
    "$P(w_i|t_{1,n}) = P(w_i|t_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34bea63-fc28-44b7-8b7f-aa87cd72bd5c",
   "metadata": {},
   "source": [
    "#### Paramètres de résolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6b0a9-a398-4d86-b33e-d4c5d02e4e28",
   "metadata": {},
   "source": [
    "Soient deux ensembles *W* et *T* pour les mots et les étiquettes. À une séquence de *n* mots est associée une séquence de *n* étiquettes de telle manière que :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    w_{1,n} &= w_1 w_2 \\ldots w_n\\\\\n",
    "    t_{1,n} &= t_1 t_2 \\ldots t_n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "L’objectif, sachant $w_{1,n}$, est de déterminer $t_{1,n}$ en maximisant $P(t_{1,n}|w_{1,n})$, c’est-à-dire, après avoir calculé toutes les probabilités possibles, de ne conserver que la plus forte :\n",
    "\n",
    "$$\n",
    "t_{1,n} = max_{t_{1,n}} P(t_{1,n}|w_{1,n})\n",
    "$$\n",
    "\n",
    "Ce qui, d’après le théorème de Bayes, s’exprime en renversant la condition :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    P(t_{1,n}|w_{1,n}) &= \\frac{P(w_{1,n}|t_{1,n}) \\cdot P(t_{1,n})}{P(w_{1,n})}\\\\\n",
    "    t_{1,n} &= \\text{max} P(w_{1,n}|t_{1,n}) \\cdot P(t_{1,n})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Il reste deux probabilités à calculer pour lesquelles on applique les hypothèses retenues plus haut :\n",
    "\n",
    "- $P(w_{1,n}|t_{1,n})$ : hypothèse selon laquelle un mot ne dépend que de son étiquette\n",
    "- $P(t_{1,n})$ : hypothèse selon laquelle une étiquette dépend de ses *n* étiquettes précédentes\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    P(w_{1,n}|t_{1,n}) &= P(w_1|t_1) \\cdot P(w_2|t_2) \\ldots P(w_n|t_n)\\\\\n",
    "    P(t_{1,n}) &= P(t_1) \\cdot P(t_2|t_1) \\cdot P(t_3|t_2) \\ldots P(t_n|t_{n−1})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e00cef-1bc6-44c0-ae3e-ccbdcbcdd2ff",
   "metadata": {},
   "source": [
    "### Modèle de Markov caché d’ordre *N*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbdadd-8c4e-4012-8de6-a013ebdbd005",
   "metadata": {},
   "source": [
    "Le modèle décrit plus haut est appelé modèle de Markov caché d’ordre 1 (*Hidden Markov Model*, ou HMM) dans la mesure où la probabilité d’apparition d’une étiquette ne dépend que de la précédente. Pour un ordre plus élevé, par exemple pour un HMM d’ordre 2, nous aurons :\n",
    "\n",
    "$$\n",
    "P(t_{1,n}) = P(t_1) \\cdot P(t_2|t_1) \\cdot P(t_3|t_1 t_2) \\ldots P(t_n|t_{n−2}t_{n−1})\n",
    "$$\n",
    "\n",
    "En appliquant un HMM, la tâche pour un étiquetage *n*-gramme se formulerait ainsi :\n",
    "\n",
    "$$\n",
    "t_{1,n} = \\text{max} P(w_1|t_1) \\cdot \\prod_{i=1,n} P(t_i|t_{i-1}) \\cdot P(w_i|t_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d26945f-1345-4e72-afb7-f3c9ea466d46",
   "metadata": {},
   "source": [
    "### Estimation des probabilités avec un corpus d’apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb229d-fa96-45aa-8f06-4cb60bf7224f",
   "metadata": {},
   "source": [
    "La première étape consiste à calculer les probabilités d’émission pour chaque mot sachant son étiquette :\n",
    "\n",
    "$$\n",
    "P(w_i|t_i) = \\frac{F(w_i, t_i)}{F(t_i)}\n",
    "$$\n",
    "\n",
    "Ensuite, on passe aux probabilités de transition :\n",
    "\n",
    "$$\n",
    "P(t_i|t_{i-1}) = \\frac{F(t_{i-1} t_i)}{F(t_{i-1})}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
