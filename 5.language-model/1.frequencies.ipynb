{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206542b2-e9cc-4dc7-81b4-d0886aaff6a4",
   "metadata": {},
   "source": [
    "# Un modèle de langage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e412a-a58e-414d-9955-148d2b3655df",
   "metadata": {},
   "source": [
    "Qu’il soit sobre (*Small Language Model*), volumineux (*Large Language Model*) ou prévu pour un domaine particulier (*Specialized Language Model*), un modèle de langage est une représentation statistique de la distribution des symboles d’une langue (lettres, phonèmes, mots…) en vue d’effectuer des prédictions. En se basant par exemple sur la fréquence d’apparition des lettres dans un corpus, il est tout à fait envisageable de générer du texte. Ou plutôt une collection de lettres. Des outils supplémentaires peuvent être intégrés au modèle, comme un lexique, ou une distribution de la longueur des mots et des phrases dans un texte.\n",
    "\n",
    "La limite principale d’un modèle de langage est qu’il est incapable de rendre compte d’une langue, tout au plus de son état à un certain moment donné en vue des paramètres initiaux qui lui ont été fournis. Il repose fondamentalement sur un corpus, qui lui-même est une extraction, et reproduira ses biais. Si votre corpus contient *La montagne de l’âme* de Gao Xingjian, il risque de donner trop d’importance aux formes conjuguées des verbes à la deuxième personne du singulier. Et si vous incluez l’ensemble du contenu d’Internet, qui lui-même est de plus en plus alimenté par des IA génératives, vous ne manquerez pas de véhiculer ses fictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8922cd54-cfda-4753-a703-5e0b3155a253",
   "metadata": {},
   "source": [
    "## Aperçu de la difficulté de la tâche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feced1c8-b974-42ce-88fd-a038fd1075ae",
   "metadata": {},
   "source": [
    "Compter des occurrences revient à subdiviser un objet en éléments et à compter le nombre de fois où chaque élément apparaît. Prenons une liste aléatoire de cent nombres entiers entre 0 et 20 et calculons combien de fois apparaît le nombre 9 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac2cb1-c0b1-419c-80ad-a0763733e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "l = [ randint(0, 20) for n in range(0, 100) ]\n",
    "\n",
    "print(f\"Le nombre 9 apparaît { l.count(9) } fois.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f9155-4b15-47b6-9cc5-a6deedce8c33",
   "metadata": {},
   "source": [
    "On peut réaliser une opération similaire en comptabilisant les apparitions de la lettre *e* dans une phrase :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086af909-da8b-41e5-ad95-652a6731472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"En pratique, un pêcheur pêche avec une canne.\"\n",
    "\n",
    "print(f\"La lettre 'e' apparaît { sent.count('e') } fois.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7732113-1c9b-483b-9e09-eb1dbc72b6e1",
   "metadata": {},
   "source": [
    "Python dissocie par défaut le caractère *e* de ses versions accentuée *ê* et majuscule *E*. Plusieurs stratégies peuvent être mises en place pour y remédier :\n",
    "\n",
    "- remplacer les caractères ;\n",
    "- additionner les occurrences de chaque cas ;\n",
    "- normaliser la phrase.\n",
    "\n",
    "Prenons la dernière stratégie en faisant appel à la méthode `.normalize()` du module `unicodedata`, qui décompose une lettre en ses différents constituants :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f754b4a6-ed57-41e2-a101-82f00cc5c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize(s):\n",
    "    \"\"\"Returns the normalized version of a string.\n",
    "\n",
    "    s -- string to normalize\n",
    "    \"\"\"\n",
    "    normalized_string = str()\n",
    "    for c in s:\n",
    "        components = unicodedata.normalize('NFKD', c)\n",
    "        base = components[0]\n",
    "        normalized_string += base.lower()\n",
    "\n",
    "    return normalized_string\n",
    "\n",
    "print(normalize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8cff1-9d5f-4c70-b4e8-b540d1086aaa",
   "metadata": {},
   "source": [
    "Grâce à la fonction `map()`, on peut obtenir le même résultat plus rapidement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8673e7d2-c345-4885-8abe-4f5409639745",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = map(lambda x: unicodedata.normalize('NFKD', x)[0], sent)\n",
    "sent = ''.join(sent)\n",
    "\n",
    "print(f\"La lettre 'e' apparaît { sent.count('e') } fois.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0d7d0a-e782-41fc-8cf4-0c0112269af1",
   "metadata": {},
   "source": [
    "Et pour basculer en bas de casse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedfcc6d-c8fa-4406-b1ea-ab251d7403cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sent.lower()\n",
    "\n",
    "print(f\"La lettre 'e' apparaît { sent.count('e') } fois.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e320609f-1868-4440-9b9c-1230b6891486",
   "metadata": {},
   "source": [
    "## Traduire le langage en probabilités"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae73b0a-55ad-440e-8d4d-c51ea6b297e9",
   "metadata": {},
   "source": [
    "Quelle est la probabilité de tirer la lettre *A* au *Scrabble* ? La question, anodine, ne peut se résoudre que si nous connaissons deux quantités :\n",
    "\n",
    "- le nombre total de jetons ;\n",
    "- le nombre de jetons *A*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1eafed-9b38-4648-b0c5-bb38ec325413",
   "metadata": {},
   "source": [
    "### Calculer des fréquences absolues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2567d7-f2a6-4519-82ef-305c7a958f4e",
   "metadata": {},
   "source": [
    "Le calcul de fréquences se trouve ainsi au cœur des modèles probabilistes et constitue souvent la première mesure statistique d’un corpus. Considérons la phrase suivante :\n",
    "\n",
    "> Le petit chat boit du lait.\n",
    "\n",
    "Et faisons l’inventaire des caractères :\n",
    "\n",
    "|Lettre|Fréquence|Cumul|\n",
    "|:-:|:-:|:-:|\n",
    "|t|5|5|\n",
    "||5|10|\n",
    "|i|3|13|\n",
    "|a|2|15|\n",
    "|e|2|17|\n",
    "|l|2|19|\n",
    "|b|1|20|\n",
    "|c|1|21|\n",
    "|d|1|22|\n",
    "|h|1|23|\n",
    "|o|1|24|\n",
    "|p|1|25|\n",
    "|u|1|26|\n",
    "|.|1|27|\n",
    "\n",
    "Réaliser la tâche en Python passe par l’objet `Counter` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd944f2c-50a5-4cb1-9dc4-0ec9eecac40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sent = \"Le petit chat boit du lait.\"\n",
    "\n",
    "unigrams = Counter(sent.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eab1e0-5999-4265-b244-0463570da173",
   "metadata": {},
   "source": [
    "Le résultat étant hérité du type `dict`, on peut accéder à la fréquence d’un caractère particulier en interrogeant la clé ou en utilisant la méthode `.get()` qui a l’avantage de ne pas lever d’exception si une clé n’est pas présente :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b43a29-0d4d-4314-8c38-bf0077596946",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    unigrams['t'],\n",
    "    unigrams.get('e'),\n",
    "    unigrams.get('j'),\n",
    "    unigrams.get('z', 0),\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225d0e0-4892-4bf5-b0c5-f0893b2b4627",
   "metadata": {},
   "source": [
    "Une autre méthode utile permet d’un coup d’œil d’afficher la liste des items les plus fréquents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9e003-31d1-4e4b-a2f9-78826f9d7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams.most_common(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402267e-3a7c-4faf-b55a-d74816aad142",
   "metadata": {},
   "source": [
    "Et pour connaître le nombre total d’occurrences ? La méthode `.values()` d’un objet `dict` permettant d’exposer la liste de toutes ses valeurs, nous pouvons ensuite facilement en établir la somme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c45bff-8650-4357-be04-18fd562a27e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sum(unigrams.values())\n",
    "\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c8a834-0638-4998-b761-2709ce0bbe43",
   "metadata": {},
   "source": [
    "### La fréquence relative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7006f548-eaed-4f1c-9265-e04fcb80a3a5",
   "metadata": {},
   "source": [
    "Le dénombrement des mots dans un texte ne donne qu’une mesure absolue de la présence de chacun, sans rien dire de leur importance. Qu’un terme apparaisse trois mille fois est en soi beaucoup, mais au milieu d’un corpus de trois milliards de mots, il ne pèse guère. D’où la nécessité de toujours considérer un chiffre parmi son environnement.\n",
    "\n",
    "Calculer la fréquence relative d’une lettre revient à diviser le nombre de fois où elle apparaît avec la taille du corpus. En probabilités, la formule revient à :\n",
    "\n",
    "$$\n",
    "P(c) = \\frac{F(c)}{N}\n",
    "$$\n",
    "\n",
    "Dans la formule, $P(c)$ est la probabilité de réalisation de l’événement $c$ – à savoir un caractère, $F(c)$ la fréquence d’apparition du caractère et $N$ la taille du corpus. En l’appliquant à la phrase plus haut, on peut en déduire que la probabilité de l’événement *i* est égale à :\n",
    "\n",
    "$$\n",
    "P(i) = \\frac{3}{27} \\approx 0.11\n",
    "$$\n",
    "\n",
    "Avec Python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ade63-606c-4e86-89c9-d941df477df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i = unigrams.get('i') / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfa1125-db62-4baa-80ff-9feccf17a712",
   "metadata": {},
   "source": [
    "Dans ce contexte, la réalisation d’un événement aléatoire est jugée indépendante des autres événements. Aussi, la probabilité de réaliser l’événement *i* puis l’événement *t* revient à multiplier les probabilités des deux événements :\n",
    "\n",
    "$$\n",
    "P(i \\cap t) = P(i) \\times P(t) = 0.11 \\times 0.18 \\approx 0.0206\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46dba25-2d1c-4ed1-89f7-b3036ab3cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i * unigrams.get('t') / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32550a3d-5279-4e36-b21a-d64577a27c87",
   "metadata": {},
   "source": [
    "### Modèle *n*-grammes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265fa94-6b41-4ad5-b928-3a064da5c627",
   "metadata": {},
   "source": [
    "Quelle serait maintenant la probabilité que l’événement *it* se produise ? La question est différente de la précédente comme on considère l’ensemble *it* comme un événement et plus comme deux événements distincts. Il est par conséquent possible d’appliquer la première formule :\n",
    "\n",
    "$$\n",
    "P(it) = \\frac{F(it)}{N-1}\n",
    "$$\n",
    "\n",
    "Pourquoi $N-1$ ? Les fréquences calculées précédemment n’impliquaient que des unigrammes (ou 1-grammes) or, ici, nous souhaitons estimer la probabilité d’un bigramme (ou 2-grammes). Caclulons le nombre de bigrammes dans la phrase :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e94bd-6f33-4526-8688-2a8e6df5e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = Counter([\n",
    "    sent[i:i+2]\n",
    "    for i in range(len(sent) - 1)\n",
    "])\n",
    "sum(bigrams.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b717b-ee6f-49a7-b181-707da99225ad",
   "metadata": {},
   "source": [
    "La règle est linéaire, de telle manière qu’il existe $N-2$ trigrammes, $N-3$ tétragrammes, $N-4$ 5-grammes etc. La probabilité de l’événement *it* s’établit ainsi à :\n",
    "\n",
    "$$\n",
    "P(it) = \\frac{3}{26} \\approx 0.1154\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc4c9e3-acf8-4d9d-9991-6f1a75ecc9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_it = bigrams['it'] / (N - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3314a-2f23-4f4b-9313-2931e2cbad7e",
   "metadata": {},
   "source": [
    "Dans un modèle *n*-grammes toutefois, la question n’est pas réellement d’estimer la probabilité d’un événement mais plutôt sa vraisemblance en fonction d’un historique. Reformulons : quelle serait la probabilité de l’événement *t* sachant que *i* est arrivé juste avant ? Du point de vue mathématique, la formule revient à :\n",
    "\n",
    "$$\n",
    "P(t|i) = \\frac{F(it)}{F(i)} = \\frac{3}{3} = 1\n",
    "$$\n",
    "\n",
    "En effet, dans le corpus, un *i* est toujours suivi d’un *t*. Vérifions avec Python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d7e5e-6b6a-4d4b-bbe8-b02ad35f0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_t_i = bigrams['it'] / unigrams['i']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ded6c-6827-4f5d-b663-1b6e67b23bf1",
   "metadata": {},
   "source": [
    "Quelle est la lettre la plus probable sachant *e* ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0073bbea-cb4a-4894-bfab-e435503d11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = unigrams.keys()\n",
    "probabilities = dict()\n",
    "for c in alphabet:\n",
    "    key = f\"e{c}\"\n",
    "    probabilities.update({\n",
    "        key: bigrams[key] / unigrams['e']\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94e8d2-9cb8-4557-986a-5c08db7050db",
   "metadata": {},
   "source": [
    "## Des contraintes pesant sur les modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf65b5-6216-41e5-a357-38eef8db2154",
   "metadata": {},
   "source": [
    "### Formule des probabilités composées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a44245c-9371-4cbe-b9b6-1551cd93c48a",
   "metadata": {},
   "source": [
    "L’intuition première pour calculer des probabilités sur des chaînes de caractères serait d’appliquer une chaîne de traitement sans historicité :\n",
    "\n",
    "$$\n",
    "P(u_1 u_2 \\ldots u_n) \\approx \\prod_i P(u_i)\n",
    "$$\n",
    "\n",
    "Pour obtenir le mot *chai*, il suffirait ainsi de multiplier entre elles les probabilités de chacune des lettres :\n",
    "\n",
    "$$\n",
    "P(chai) = P(c) \\cdot P(h) \\cdot P(a) \\cdot P(i) \\approx 6.5649 \\times 10^{-5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65349fee-f87c-4272-8217-802dbca6e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a unigram model, what are the odds to get the word 'chai'?\n",
    "word = \"chai\"\n",
    "p_chai = 1\n",
    "for c in word:\n",
    "    p_chai *= unigrams[c] / N\n",
    "p_chai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9faa75-43af-400a-8b72-abbe5480e73a",
   "metadata": {},
   "source": [
    "Et dans un modèle bigramme, la logique reste la même, chaque unité étant cette fois-ci composé de deux caractères :\n",
    "\n",
    "$$\n",
    "P(chai) = P(ch) \\cdot P(ai) = 0.0016\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ca613-d640-40b2-974b-f1a368867601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and in a bigram model?\n",
    "p_chai = 1\n",
    "for i in range(0, len(word), 2):\n",
    "    bigram = word[i:i+2]\n",
    "    p_chai *= bigrams[bigram] / (N - 1)\n",
    "p_chai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3399fef4-8f88-4bd3-a9d1-4681d163c952",
   "metadata": {},
   "source": [
    "Ce genre de modèle qui ne tient pas compte du contexte reste très limité : un générateur basé sur une telle règle écrirait des caractères aléatoirement. Après tout, les chaînes *du* et *hb* bénéficient de la même estimation de probabilité :\n",
    "\n",
    "$$\n",
    "P(du) = P(d) * P(u) \\approx 0.0384 \\cdot 0.0384 \\approx 0.001479\\\\\n",
    "P(hb) = P(h) * P(b) \\approx 0.0384 \\cdot 0.0384 \\approx 0.001479\n",
    "$$\n",
    "\n",
    "Une meilleure approche consiste à poser le problème sous forme de probabilités conditionnelles :\n",
    "\n",
    "$$\n",
    "P(chai) = P(h|c) \\cdot P(a|ch) \\cdot P(i|cha)\n",
    "$$\n",
    "\n",
    "Une formule qui peut se généraliser :\n",
    "\n",
    "$$\n",
    "P(w_1 w_2 \\ldots w_n) = \\prod_{i=1} P(w_i ∣ w_0 \\ldots w_{i−1})\n",
    "$$\n",
    "\n",
    "Si elle semble prometteuse, sur l’argument qu’elle repose toujours sur l’antériorité, elle aboutit très rapidement à une probabilité de 0 pour des énoncés pourtant probables :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(chai) &= P(h|c) \\cdot P(a|ch) \\cdot P(i|cha)\\\\\n",
    "&= \\frac{F(ch)}{F(c)} \\cdot \\frac{F(cha)}{F(ch)} \\cdot \\frac{F(chai)}{F(cha)}\\\\\n",
    "&= \\frac{1}{1} \\cdot \\frac{1}{1} \\cdot \\frac{0}{1}\\\\\n",
    "&= 1 \\cdot 1 \\cdot 0\\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4bdd02-4160-4dbd-ba16-19239c792805",
   "metadata": {},
   "source": [
    "### Hypothèses de correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb7534-d311-4974-aae5-6b0e5f2c991b",
   "metadata": {},
   "source": [
    "Pour éviter cet écueil, la première intuition serait d’ignorer tout simplement les cas qui n’apparaissent pas dans le corpus d’entraînement. Si nous retenons cette idée, $P(chai)$ devient subitement égale à 1. Passer de 0 à 100 % de chances de voir le mot *chai* (autant que le mot *chat*) semble indiquer que notre modèle de langage n’est pas très rationnel.\n",
    "\n",
    "Une correction plus envisageable, et qui prend le nom de **lissage de Laplace**, serait d’augmenter de 1 toutes les fréquences du corpus et d’assigner la même valeur aux évènements nuls :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(chat) &= \\frac{F(ch)+1}{F(c)+1} \\cdot \\frac{F(cha)+1}{F(ch)+1} \\cdot \\frac{F(chat)+1}{F(cha)+1}\\\\\n",
    "&= \\frac{2}{2} \\cdot \\frac{2}{2} \\cdot \\frac{2}{2}\\\\\n",
    "&= 1\\\\\n",
    "P(chai) &= \\frac{F(ch)+1}{F(c)+1} \\cdot \\frac{F(cha)+1}{F(ch)+1} \\cdot \\frac{F(chai)+1}{F(cha)+1}\\\\\n",
    "&= \\frac{2}{2} \\cdot \\frac{2}{2} \\cdot \\frac{1}{2}\\\\\n",
    "&= 0.5\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "En poursuivant cette logique, une meilleure approximation de la probabilité d’un évènement inconnu s’appuierait sur une pondération du maximum de vraisemblance. Fixons le paramètre $\\alpha$ à 0,05 pour la probabilité d’un évènement inconnu et un paramètre $k$ pour la pondération qui vaut $1 - \\alpha$ soit 0,95 :\n",
    "\n",
    "$$\n",
    "P(chai) = 0.95 \\times 0 + \\frac{0.05}{27} \\approx 0.0019\n",
    "$$\n",
    "\n",
    "La formule vaut ainsi :\n",
    "\n",
    "$$\n",
    "P(w_i) = k \\cdot P_{ML}(w_i) + \\frac{\\alpha}{N}\n",
    "$$\n",
    "\n",
    "Dans la réalité, le paramètre $k$ est ajusté sur le corpus d’entraînement plutôt que déterminé arbitrairement. D’autres techniques existent, comme les actualisations de Good-Turing et de Witten-Bell, ou encore le lissage de Kneser-Ney qui estiment les évènements inconnus sur la base des hapax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8834a41c-e086-40c2-a918-ac065042ebc7",
   "metadata": {},
   "source": [
    "### L’hypothèse markovienne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a889af95-1909-4474-84e6-6f3161ddd33b",
   "metadata": {},
   "source": [
    "En théorie, la formule des probabilités composées couvre bien les besoins impliqués par la construction d’un modèle de langage simple. Après tout, quand un corpus d’apprentissage n’est constitué que de quelques caractères et que le nombre de caractères d’un mot est fini, il semble envisageable de calculer toutes les possibilités.\n",
    "\n",
    "Étendons notre exemple au français, qui dispose d’un alphabet de 26 lettres et dont le mot le plus long, *anticonstitutionnellement*, en compte 25. Là encore, générer un mot de *n* caractères en partant d’un état initial et en se fondant sur des données d’entraînement suffisamment importantes semble à portée de main. Oui, mais… qu’en est-il des mots composés ? Et dans les autres langues ? Les règles de composition de l’allemand font par exemple le bonheur des écoliers qui inventent les péripéties du capitaine naviguant sur le Danube et le Rhin (*Donaurheinschifffahrtskapitän*) afin d’égaler la prouesse établie par la *Donau­dampf­schiffahrts­elektrizitäten­haupt­betriebs­werk­bau­unter­beamten­gesellschaft*.\n",
    "\n",
    "Rappelons aussi que l’unité de base des modèles de langage n’est pas le caractère mais le mot. L’alphabet devient lexique et la longueur d’une phrase étant virtuellement infinie, les besoins en calcul sont si délirants qu’il ne sera jamais possible d’estimer de telles probabilités.\n",
    "\n",
    "Comment faire alors ? C’est ici qu’intervient l’hypothèse markovienne, appelée ainsi après le mathématicien russe Andreï Markov, qui suggère que la probabilité conditionnelle d’un mot sachant son historique est approximée par la probabilité de ce mot sachant uniquement celui qui le précède. Par exemple, la probabilité que le mot *vagues* termine la phrase *Le soleil brille au-dessus des* est proche de celle que le mot *vagues* suive directement l’article *des* :\n",
    "\n",
    "$$\n",
    "P(\\text{flots}|\\text{Le soleil brille au-dessus des}) \\approx P(\\text{flots}|\\text{des})\n",
    "$$\n",
    "\n",
    "On parle aussi d’**horizon limité**, une approximation qui peut s’écrire sous la forme :\n",
    "\n",
    "$$\n",
    "P(w_{n+1}|w_1 \\ldots w_n) \\approx P(w_{n+1}|w_n)\n",
    "$$\n",
    "\n",
    "Mieux, l’hypothèse markovienne dans sa généralisation définit une fenêtre contextuelle à gauche :\n",
    "\n",
    "$$\n",
    "P(w_1 w_2 \\ldots w_n) \\approx \\prod_i P(w_i|w_{i-k} \\ldots w_{i-1})\n",
    "$$\n",
    "\n",
    "Les modèles probabilistes peuvent s’étendre ainsi aux trigrammes, aux tétragrammes, aux… sauf que le langage fait montre de tant de diversité que les modèles *n*-grammes ne suffisent pas. Prenons un énoncé :\n",
    "\n",
    "> Le film, encensé par la critique de nombreux médias spécialisés, comme *Les inrockuptibles*, *Télérama*, *ÉcranLarge* ou encore *Première*, qui s’était déjà exprimée favorablement sur l’opus précédent, est fort mauvais.\n",
    "\n",
    "La structure des phrases, notamment à l’écrit, implique souvent une dépendance longue distance (*long distance dependancy* ou LDD) facilitée par des incises, des relatives ou des tournures interrogatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
